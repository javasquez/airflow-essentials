# ğŸš€ Orchestration with Airflow â€“ Essential DAG Examples

This project demonstrates how to orchestrate data pipelines using **Apache Airflow 2.11.0**, focusing on practical and foundational use cases. It runs using the **official Docker Compose setup**, and includes integration with an **external SQL Server database**, plus examples using `pandas`, XComs, Bash scripts, and more.

---

## ğŸ“¦ Tech Stack

| Component           | Technology                       |
|---------------------|----------------------------------|
| Orchestrator        | Apache Airflow 2.11.0            |
| Executor            | CeleryExecutor                   |
| Deployment          | Docker Compose (official)        |
| Metadata Database   | PostgreSQL (default in Docker)   |
| External Database   | Microsoft SQL Server (via RDS)   |
| Language            | Python 3.12                      |
| Airflow Providers   | `apache-airflow-providers-microsoft-mssql` |

---

## ğŸ“ Project Structure

ORCHESTRATION-WITH-AIRFLOW/
â”‚
â”œâ”€â”€ config/                          # Custom configuration files (optional)
â”‚
â”œâ”€â”€ dags/                            # All DAG definitions
â”‚   â”œâ”€â”€ bash_scripts/                # Bash scripts used in BashOperator
â”‚   â”‚   â””â”€â”€ taskA.sh
â”‚   â”œâ”€â”€ dag_python_branching_catchup.py      # DAG with catchup and weekday branching
â”‚   â”œâ”€â”€ dag_python_branching_operator.py     # DAG using BranchPythonOperator + Variable
â”‚   â”œâ”€â”€ dag_python_branching_taskgroup.py    # DAG using TaskGroup to group related tasks
â”‚   â”œâ”€â”€ dag_python_database.py               # DAG that interacts with external SQL Server
â”‚   â”œâ”€â”€ dag_python_operator.py               # Basic PythonOperator example with kwargs
â”‚   â”œâ”€â”€ dag_python_pipeline.py               # pandas pipeline: read, clean, group
â”‚   â”œâ”€â”€ dag_xcoms.py                         # DAG demonstrating XCom usage
â”‚   â”œâ”€â”€ dag.py                               # Generic starter DAG
â”‚   â””â”€â”€ modern_dag.py                        # DAG with context manager and Bash script
â”‚
â”œâ”€â”€ data/                           # Local input/output data for the DAGs
â”‚   â”œâ”€â”€ inputs/
â”‚   â”‚   â””â”€â”€ insurance.csv            # Source CSV used in pandas pipeline
â”‚   â””â”€â”€ outputs/
â”‚       â””â”€â”€ smokers.csv             # Output generated from pipeline
â”‚
â”œâ”€â”€ database/                        # Optional: custom database scripts or metadata
â”‚
â”œâ”€â”€ logs/                            # Airflow logs (auto-generated at runtime)
â”‚
â”œâ”€â”€ plugins/                         # Place for custom operators, hooks, macros
â”‚
â”œâ”€â”€ venv/                            # Python virtual environment (optional if using system Python)
â”‚
â”œâ”€â”€ .env                             # Environment variables (used by docker-compose)
â”œâ”€â”€ docker-compose.yaml              # Official multi-service Docker setup for Airflow
â””â”€â”€ README.md                        # Documentation for the project




---

## âœ… DAGs Overview

| DAG File                              | Description                                                               |
|--------------------------------------|---------------------------------------------------------------------------|
| `dag_python_operator.py`             | Basic PythonOperator with parameters                                       |
| `dag_python_database.py`             | Executes SQL commands on SQL Server using `SQLExecuteQueryOperator`       |
| `dag_python_pipeline.py`             | Pandas pipeline: reads, cleans, and transforms CSV                        |
| `dag_python_branching_operator.py`   | Branches logic based on Airflow Variables using `BranchPythonOperator`    |
| `dag_python_branching_catchup.py`    | Demonstrates `catchup=True` with date logic for weekday/weekend branching |
| `dag_python_branching_taskgroup.py`  | Groups related tasks using `TaskGroup`                                    |
| `dag_xcoms.py`                       | Uses XCom to pass data between tasks                                      |
| `modern_dag.py`                      | Uses context manager + BashOperator with external script                  |
| `dag.py`                             | A generic starter DAG                                                     |

---

## ğŸ”Œ Airflow Connection Required

Create the following connection in the Airflow UI:

- **Connection ID:** `sql_server_conn`
- **Conn Type:** Microsoft SQL Server
- **Host:** your SQL Server hostname or RDS endpoint
- **Port:** 1433
- **Schema:** dbo
- **Login/Password:** your credentials
- **Extra:**
  ```json
  {
    "encrypt": "no",
    "trustServerCertificate": "yes"
  }
